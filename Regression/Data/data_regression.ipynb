{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import refexps\n",
    "from os import listdir, path\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pickle\n",
    "import compound_splitter\n",
    "from refexps import load_data, move_level, gamerun_level, get_previous #, lex_sim\n",
    "from os import listdir, path\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import compound_splitter\n",
    "from scipy import stats\n",
    "from nltk import word_tokenize\n",
    "from string import punctuation\n",
    "punct = list(punctuation)\n",
    "\n",
    "dataset_dir = '../En_De_Dataset/All/RefExp'\n",
    "success_dir = '../En_De_Dataset/All/Success'\n",
    "\n",
    "def exp_len(exp, language, splits, compound):\n",
    "    '''\n",
    "    Extract content words by the phrase\n",
    "    :param exp: referring expression tuple\n",
    "    :return: set of content words of the phrase\n",
    "    '''\n",
    "    exp = exp[2]\n",
    "    words = word_tokenize(exp, language)\n",
    "    if compound == True:\n",
    "        if language == 'german':\n",
    "            words_nocomp = []\n",
    "            for w in words:\n",
    "                word = compound_splitter.split_word(w, splits)\n",
    "                words_nocomp = words_nocomp + (word.split(' '))\t\n",
    "                words = words_nocomp\n",
    "    return len(words)\n",
    "\n",
    "def len_div(exp1, previous, language, splits, compound):\n",
    "    '''\n",
    "    compute dice similarity between the content words in two referring expression\n",
    "    :param exp1: referring expression tuple 1\n",
    "    :param exp2: referring expression tuple 2\n",
    "    :return: dice similarity between the content words in the expressions; undefined if they have none\n",
    "    '''        \n",
    "    len_exp = exp_len(exp1, language, splits, compound)    \n",
    "    previous_len = np.mean(previous)\n",
    "    metric = float(previous_len - len_exp)/float(previous_len + len_exp)\n",
    "    return metric if not np.isnan(metric) and metric > -1  and metric < 1 else 'undefined'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184 164\n"
     ]
    }
   ],
   "source": [
    "structure = '[english-->{move_id:(success, confid, tuple_len, tuple_lex, tuple_form}...., german ---> {}'\n",
    "tuple_structure = '(general(list), p(list), e(list))'\n",
    "def data_values_move(refexps_dir, success_dir, compound = True):\n",
    "    splits = compound_splitter.load_dict('../de_lower.dict')\n",
    "    data_en = defaultdict(dict)\n",
    "    data_de = defaultdict(dict)\n",
    "    data_regression = []\n",
    "    corr_data_lex = defaultdict(list)\n",
    "    corr_data_form =defaultdict(list)\n",
    "    success_types = set()\n",
    "    confid_types = set()\n",
    "    moves = 0\n",
    "    moves_regr = 0\n",
    "    for dialogue in listdir(refexps_dir):\n",
    "        #extract dialogue type\n",
    "        success_file = dialogue.replace('refexps', 'movesuccess')\n",
    "        success_file = path.join(success_dir, success_file)\n",
    "        if dialogue.startswith('FTT'):\n",
    "            language = 'german'\n",
    "        else:\n",
    "            language = 'english'\n",
    "        dialogue_path = path.join(refexps_dir, dialogue)\n",
    "        #load referring expressions data\n",
    "        dialogue_path = refexps.load_data(dialogue_path)\n",
    "        success_moves = refexps.load_data(success_file)\n",
    "        #move level\n",
    "        dialogue_move = refexps.move_level(dialogue_path)\n",
    "        #iterate over moves\n",
    "        for move in dialogue_move:\n",
    "            move_id = dialogue + str(move[0])\n",
    "            success = success_moves[move[0]][0]\n",
    "            confid = success_moves[move[0]][1]\n",
    "            refs = move[1]\n",
    "            if success == 'success':\n",
    "                success = 'correct'\n",
    "            if success == 'wrong':\n",
    "                success = 'incorrect'\n",
    "            if success == 'na':\n",
    "                success = 'no_video'\n",
    "            success_types.add(success)\n",
    "            if confid == 'confid':\n",
    "                confid = 'confident'\n",
    "            if confid == 'inconf':\n",
    "                condif = 'unconfident'\n",
    "            if confid == 'reconf':\n",
    "                confid = 'reconfirm'\n",
    "            confid_types.add(confid)\n",
    "            \n",
    "            form_values = ([], [], [])\n",
    "            len_values = ([], [], [])\n",
    "            lex_values = ([], [], [])\n",
    "            \n",
    "            for obj in refs:\n",
    "                refobj = refs[obj]\n",
    "                prev_length = []\n",
    "                speakers = set()\n",
    "                \n",
    "                divs = []\n",
    "                divs_speakers = defaultdict(list)\n",
    "                \n",
    "                total_words = 0\n",
    "                total_words_speaker = defaultdict(int)\n",
    "                sameword_n = 0\n",
    "                sameword_n_speaker = defaultdict(int)\n",
    "                prev_words = set()\n",
    "                \n",
    "                total_n = len(refs[obj])\n",
    "                sameform_n = 0\n",
    "                sameform_n_speaker = defaultdict(int)\n",
    "                total_n_speaker = defaultdict(int)\n",
    "                prev_forms = set()\n",
    "                for ref in refobj:\n",
    "                    speaker = ref[1]\n",
    "                    speakers.add(speaker)\n",
    "                    #LENGTH\n",
    "                    if prev_length != []:\n",
    "                        div = len_div(ref, prev_length, language, splits, compound)\n",
    "                        if div != 'undefined':\n",
    "                            divs.append(div)\n",
    "                            divs_speakers[speaker].append(div)\n",
    "                    len_exp = exp_len(ref, language, splits, compound) \n",
    "                    prev_length.append(len_exp)\n",
    "                    #LEXICAL\n",
    "                    words = refexps.lex_material(ref, language, splits, compound)\n",
    "                    if prev_words != set():\n",
    "                        for w in words:\n",
    "                            if w in prev_words:\n",
    "                                sameword_n += 1\n",
    "                                sameword_n_speaker[speaker] += 1\n",
    "                    else:\n",
    "                        starter = speaker\n",
    "                        first_words = len(words)\n",
    "                    total_words_speaker[speaker] += len(words)\n",
    "                    total_words += len(words)\n",
    "                    for w in words:\n",
    "                        prev_words.add(w)\n",
    "                    #FORM \n",
    "                    form = ref[3]\n",
    "                    if prev_forms != set():\n",
    "                        if form in prev_forms:\n",
    "                            sameform_n += 1\n",
    "                            sameform_n_speaker[speaker] += 1\n",
    "                    else:\n",
    "                        starter = speaker\n",
    "                    total_n_speaker[speaker] += 1\n",
    "                    prev_forms.add(form)  \n",
    "                #LENGTH\n",
    "\n",
    "                if divs != []:\n",
    "                    alignment = np.mean(divs)\n",
    "                    len_values[0].append(alignment)\n",
    "                for s in speakers:\n",
    "                    if divs_speakers[speaker] != []:\n",
    "                        alignment = np.mean(divs_speakers[speaker])\n",
    "                        if s == 'p-utts':\n",
    "                            len_values[1].append(alignment)\n",
    "                        if s == 'e-utts':\n",
    "                            len_values[2].append(alignment)\n",
    "                #LEXICAL\n",
    "                lexical_align = False\n",
    "                lexical_align_p = False\n",
    "                lexical_align_e = False\n",
    "                if (total_words - first_words) != 0:\n",
    "                    alignment_lex = float(sameword_n)/float(total_words - first_words)\n",
    "                    lex_values[0].append(alignment_lex)\n",
    "                    lexical_align = True\n",
    "                for s in speakers:\n",
    "                        if starter == s:\n",
    "                            if (total_words_speaker[s] - first_words) != 0:\n",
    "                                alignment = float(sameword_n_speaker[s])/float(total_words_speaker[s] - first_words)\n",
    "                                if s == 'p-utts':\n",
    "                                    lex_values[1].append(alignment)\n",
    "                                    alignment_lex_p = alignment\n",
    "                                    lexical_align_p = True\n",
    "                                if s == 'e-utts':\n",
    "                                    lex_values[2].append(alignment)\n",
    "                                    lexical_align_e = True\n",
    "                                    alignment_lex_e = alignment\n",
    "                        else:\n",
    "                            if(total_words_speaker[s]) != 0:\n",
    "                                alignment = float(sameword_n_speaker[s])/float(total_words_speaker[s])\n",
    "                                if s == 'p-utts':\n",
    "                                    lex_values[1].append(alignment)\n",
    "                                    lexical_align_p = True\n",
    "                                    alignment_lex_p = alignment\n",
    "                                if s == 'e-utts':\n",
    "                                    lex_values[2].append(alignment)\n",
    "                                    lexical_align_e = True\n",
    "                                    alignment_lex_e = alignment\n",
    "                                    \n",
    "                #FORM\n",
    "                \n",
    "                if (total_n - 1) != 0:\n",
    "                    alignment = float(sameform_n)/float(total_n - 1)\n",
    "                    form_values[0].append(alignment)\n",
    "                    if lexical_align == True:\n",
    "                        corr_data_form['alignment'].append(alignment)\n",
    "                        corr_data_lex['alignment'].append(alignment_lex)\n",
    "                for s in speakers:\n",
    "                        if starter == s:\n",
    "                            if (total_n_speaker[s] - 1) != 0:\n",
    "                                alignment = float(sameform_n_speaker[s])/float(total_n_speaker[s] - 1)\n",
    "                                if s == 'p-utts':\n",
    "                                    form_values[1].append(alignment)\n",
    "                                    if lexical_align_p == True:\n",
    "                                        corr_data_form['alignment p'].append(alignment)\n",
    "                                        corr_data_lex['alignment p'].append(alignment_lex)\n",
    "                                if s == 'e-utts':\n",
    "                                    form_values[2].append(alignment)\n",
    "                                    if lexical_align_e == True:\n",
    "                                        corr_data_form['alignment e'].append(alignment)\n",
    "                                        corr_data_lex['alignment e'].append(alignment_lex)\n",
    "                        else:\n",
    "                            if(total_n_speaker[s]) != 0:\n",
    "                                alignment = float(sameform_n_speaker[s])/float(total_n_speaker[s])\n",
    "                                if s == 'p-utts':\n",
    "                                    form_values[1].append(alignment)\n",
    "                                    if lexical_align_p == True:\n",
    "                                        corr_data_form['alignment p'].append(alignment)\n",
    "                                        corr_data_lex['alignment p'].append(alignment_lex)\n",
    "                                if s == 'e-utts':\n",
    "                                    form_values[2].append(alignment)\n",
    "                                    if lexical_align_e == True:\n",
    "                                        corr_data_form['alignment e'].append(alignment)\n",
    "                                        corr_data_lex['alignment e'].append(alignment_lex)\n",
    "                 \n",
    "            tuple_values = (success, confid, len_values, lex_values, form_values)\n",
    "            moves += 1\n",
    "    \n",
    "            if success == 'no_video':\n",
    "                pass\n",
    "            else:\n",
    "                moves_regr += 1\n",
    "                if language == 'german':\n",
    "                    data_de[move_id] = tuple_values\n",
    "                else:\n",
    "                    data_en[move_id] = tuple_values\n",
    "    print moves, moves_regr\n",
    "    data_regression = [data_de, data_en]\n",
    "    return data_regression, corr_data_lex, corr_data_form\n",
    "'''\n",
    "move_level_data = data_values_move(dataset_dir, success_dir)\n",
    "data_regression = {}\n",
    "for v in move_level_data:\n",
    "    data_regression[v] = move_level_data[v].values()\n",
    "pickle.dump(data_regression, open('regression_lexical.p', 'w'))\n",
    "'''\n",
    "data_regression, corr_data_lex, corr_data_form = data_values_move(dataset_dir, success_dir)\n",
    "pickle.dump(data_regression, open('data_regression.p', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alignment e\n",
      "True\n",
      "(0.30125736010733756, 3.4471455691122793e-06)\n",
      "Significant\n",
      "alignment p\n",
      "True\n",
      "(0.22717282186382926, 2.5584311245288579e-06)\n",
      "Significant\n",
      "alignment\n",
      "True\n",
      "(0.27381738359149027, 9.7695108099482766e-10)\n",
      "Significant\n"
     ]
    }
   ],
   "source": [
    "for v in corr_data_form:\n",
    "    print v\n",
    "    print len(corr_data_form[v]) == len(corr_data_lex[v])\n",
    "    print stats.pearsonr(corr_data_form[v], corr_data_lex[v])\n",
    "    if stats.pearsonr(corr_data_form[v], corr_data_lex[v])[1] < 0.05:\n",
    "        print 'Significant'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
